{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Zoom Camp 2022\n",
    "### Week 8 assignment\n",
    "\n",
    "In this homework, we'll build a model for predicting if we have an image of a dino or a dragon. For this, we will use the \"Dino or Dragon?\" dataset that can be downloaded from [Kaggle](https://www.kaggle.com/datasets/agrigorev/dino-or-dragon). \n",
    "\n",
    "The dataset contains around 1900 images of dinos and around 1900 images of dragons. \n",
    "\n",
    "The dataset contains separate folders for training and testing.\n",
    "\n",
    "I have chosen the first 20% of the pictures in the train folder to be in the validation folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "For this homework we will use Convolutional Neural Network (CNN). Like in the lectures, we'll use Keras.\n",
    "\n",
    "You need to develop the model with following structure:\n",
    "\n",
    "* The shape for input should be `(150, 150, 3)`\n",
    "* Next, create a convolutional layer ([`Conv2D`](https://keras.io/api/layers/convolution_layers/convolution2d/)):\n",
    "    * Use 32 filters\n",
    "    * Kernel size should be `(3, 3)` (that's the size of the filter)\n",
    "    * Use `'relu'` as activation \n",
    "* Reduce the size of the feature map with max pooling ([`MaxPooling2D`](https://keras.io/api/layers/pooling_layers/max_pooling2d/))\n",
    "    * Set the pooling size to `(2, 2)`\n",
    "* Turn the multi-dimensional result into vectors using a [`Flatten`](https://keras.io/api/layers/reshaping_layers/flatten/) layer\n",
    "* Next, add a `Dense` layer with 64 neurons and `'relu'` activation\n",
    "* Finally, create the `Dense` layer with 1 neuron - this will be the output\n",
    "    * The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "\n",
    "As optimizer use [`SGD`](https://keras.io/api/optimizers/sgd/) with the following parameters:\n",
    "\n",
    "* `SGD(lr=0.002, momentum=0.8)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.1, size_inner=100):\n",
    "    inputs = keras.Input(shape=(150, 150, 3))\n",
    "    conv_out = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(inputs)\n",
    "    pool_out = layers.MaxPooling2D(pool_size=(2, 2))(conv_out)\n",
    "    vectors = layers.Flatten()(pool_out)\n",
    "    inner = layers.Dense(size_inner, activation='relu')(vectors)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(inner)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.8)\n",
    "    loss = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.002\n",
    "dense = 64\n",
    "model = make_model(learning_rate=lr, size_inner=dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Since we have a binary classification problem, what is the best loss function for us?\n",
    "- `binary crossentropy`\n",
    "- `focal loss`\n",
    "- `mean squared error`\n",
    "- `categorical crossentropy`\n",
    "\n",
    "Note: since we specify an activation for the output layer, we don't need to set from_logits=True\n",
    "\n",
    "Answer: should be BinaryCrossentropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What's the total number of parameters of the model? You can use the `summary` method for that. \n",
    "\n",
    "- 9215873\n",
    "- 11215873\n",
    "- 14215873\n",
    "- 19215873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 74, 74, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 175232)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                11214912  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,215,873\n",
      "Trainable params: 11,215,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the Total params value - 11215873"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators and Training\n",
    "\n",
    "For the next two questions, use the following data generator for both train and validation:\n",
    "\n",
    "```python\n",
    "ImageDataGenerator(rescale=1./255)\n",
    "```\n",
    "\n",
    "* We don't need to do any additional pre-processing for the images.\n",
    "* When reading the data from train/val directories, check the `class_mode` parameter. Which value should it be for a binary classification problem?\n",
    "* Use `batch_size=20`\n",
    "* Use `shuffle=True` for both training and validation \n",
    "\n",
    "For training use `.fit()` with the following params:\n",
    "\n",
    "```python\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1594 images belonging to 2 classes.\n",
      "Found 1594 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    '..\\\\data\\\\train',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    '..\\\\data\\\\train',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dino': 0, 'dragon': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 [==============================] - 37s 456ms/step - loss: 0.6751 - accuracy: 0.6142 - val_loss: 0.6193 - val_accuracy: 0.7384\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 34s 431ms/step - loss: 0.5882 - accuracy: 0.7133 - val_loss: 0.5070 - val_accuracy: 0.8181\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 34s 431ms/step - loss: 0.4953 - accuracy: 0.7754 - val_loss: 0.4318 - val_accuracy: 0.8206\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 35s 435ms/step - loss: 0.4291 - accuracy: 0.8124 - val_loss: 0.3651 - val_accuracy: 0.8607\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 35s 442ms/step - loss: 0.3675 - accuracy: 0.8413 - val_loss: 0.3205 - val_accuracy: 0.8883\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 35s 441ms/step - loss: 0.3259 - accuracy: 0.8683 - val_loss: 0.2937 - val_accuracy: 0.8852\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 34s 432ms/step - loss: 0.2980 - accuracy: 0.8858 - val_loss: 0.2831 - val_accuracy: 0.8921\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 36s 448ms/step - loss: 0.2652 - accuracy: 0.8965 - val_loss: 0.2285 - val_accuracy: 0.9197\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 34s 428ms/step - loss: 0.2378 - accuracy: 0.9134 - val_loss: 0.1954 - val_accuracy: 0.9410\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 34s 431ms/step - loss: 0.2237 - accuracy: 0.9090 - val_loss: 0.1746 - val_accuracy: 0.9435\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What is the median of training accuracy for all the epochs for this model?\n",
    "\n",
    "- 0.40\n",
    "- 0.60\n",
    "- 0.90\n",
    "- 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8867628574371338"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8547678887844086"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 4\n",
    "\n",
    "What is the standard deviation of training loss for all the epochs for this model?\n",
    "\n",
    "- 0.11\n",
    "- 0.66\n",
    "- 0.99\n",
    "- 0.33\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13467390660073522"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14583254807767554"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Augmentation\n",
    "\n",
    "For the next two questions, we'll generate more data using data augmentations. \n",
    "\n",
    "Add the following augmentations to your training data generator:\n",
    "\n",
    "* `rotation_range=40,`\n",
    "* `width_shift_range=0.2,`\n",
    "* `height_shift_range=0.2,`\n",
    "* `shear_range=0.2,`\n",
    "* `zoom_range=0.2,`\n",
    "* `horizontal_flip=True,`\n",
    "* `fill_mode='nearest'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1594 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen_aug = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    ")\n",
    "\n",
    "train_generator_aug = train_datagen_aug.flow_from_directory(\n",
    "    '..\\\\data\\\\train',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 5 \n",
    "\n",
    "Let's train our model for 10 more epochs using the same code as previously.\n",
    "Make sure you don't re-create the model - we want to continue training the model\n",
    "we already started training.\n",
    "\n",
    "What is the mean of validation loss for all the epochs for the model trained with augmentations?\n",
    "\n",
    "- 0.15\n",
    "- 0.77\n",
    "- 0.37\n",
    "- 0.97\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 [==============================] - 35s 433ms/step - loss: 0.1833 - accuracy: 0.9341 - val_loss: 0.1466 - val_accuracy: 0.9611\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 34s 428ms/step - loss: 0.1572 - accuracy: 0.9504 - val_loss: 0.2644 - val_accuracy: 0.8683\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 34s 431ms/step - loss: 0.1340 - accuracy: 0.9592 - val_loss: 0.1107 - val_accuracy: 0.9793\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 34s 431ms/step - loss: 0.1251 - accuracy: 0.9636 - val_loss: 0.0884 - val_accuracy: 0.9843\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 34s 429ms/step - loss: 0.0977 - accuracy: 0.9774 - val_loss: 0.0853 - val_accuracy: 0.9856\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 34s 431ms/step - loss: 0.0906 - accuracy: 0.9780 - val_loss: 0.0689 - val_accuracy: 0.9937\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 35s 433ms/step - loss: 0.0739 - accuracy: 0.9849 - val_loss: 0.0490 - val_accuracy: 0.9944\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 35s 435ms/step - loss: 0.0594 - accuracy: 0.9900 - val_loss: 0.0457 - val_accuracy: 0.9975\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 35s 438ms/step - loss: 0.0462 - accuracy: 0.9956 - val_loss: 0.0784 - val_accuracy: 0.9881\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 35s 433ms/step - loss: 0.0449 - accuracy: 0.9937 - val_loss: 0.0289 - val_accuracy: 0.9994\n"
     ]
    }
   ],
   "source": [
    "history_aug = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09662165641784667"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(history_aug.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 6\n",
    "\n",
    "What's the average of validation accuracy for the last 5 epochs (from 6 to 10)\n",
    "for the model trained with augmentations?\n",
    "\n",
    "- 0.84\n",
    "- 0.54\n",
    "- 0.44\n",
    "- 0.24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9946047782897949"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(history_aug.history['val_accuracy'][-5:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40a089c9e2876b59c14a032651a8ac874d98378ee6e08a1496bf9fa8d864ff0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
